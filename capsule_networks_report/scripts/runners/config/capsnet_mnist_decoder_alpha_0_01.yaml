data_provider_name: mnist
batch_size: 50
epochs: 100
early_stop_after_n_epochs: 10
optimizer_kwargs:
  learning_rate: 0.0001
layers:
- type: conv
  filter_count: 256
  kernel_length: 9
  strides: 1
  activation: leaky_relu
- type: caps_primary
  caps_count: 32
  caps_dim: 8
  kernel_length: 9
  strides: 2
  activation: leaky_relu
- type: caps_out
  caps_count: 5
  caps_dim: 16
- type: caps_routing
  iterations: 2
  id: caps_routing_out
- type: caps_pred
  id: caps_prediction
  assert_output_shape: []
- type: caps_masking
  routing_capsule_input_id: caps_routing_out
  id: mask

- type: softmax_pred
  num_classes: 10
  id: softmax
- type: argmax
  id: prediction

# do a dense layer so the mask can distribute
- input_id: mask
  type: reshape
  shape: [50, 80]
- type: dense
  units: 256
  activation: leaky_relu
- type: dense
  units: 784
  activation: leaky_relu
# flatten so it works like before. later i can change it so the loss accepts squares!
- type: reshape
  shape: [50, 784]
  id: flattened_decoder_output

loss:
  type: reconstruction
  decoder_output_id: flattened_decoder_output
  softmax_output_id: softmax
  reconstruction_weight: 0.01
prediction_id: prediction
