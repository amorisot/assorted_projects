data_provider_name: mnist
batch_size: 50
epochs: 100
early_stop_after_n_epochs: 10
optimizer_kwargs:
  learning_rate: 0.0001
layers:
- type: conv
  filter_count: 256
  kernel_length: 9
  strides: 1
  activation: leaky_relu
- type: max_pool
  pool_size: [2, 2]
  strides: 2
- type: conv
  filter_count: 16
  kernel_length: 1
  strides: 1
  activation: leaky_relu
- type: reshape
  shape: [-1, 1600]
- type: dense
  units: 16
  id: flat_mask
  activation: leaky_relu

- type: softmax_pred
  num_classes: 10
  id: softmax
- type: argmax
  id: prediction

# do a dense layer so the mask can distribute
- input_id: flat_mask
  type: dense
  units: 256  # 2 * 2 * 64
  activation: leaky_relu
- type: reshape
  shape: [-1, 2, 2, 64]
- type: upscale
  method: nearest_neighbors
  shape: [8, 8]
  assert_output_shape: [8, 8, 64]
- type: deconv
  filter_size: 3
  filter_count: 32
  activation: leaky_relu
- type: upscale
  method: nearest_neighbors
  shape: [16, 16]
- type: deconv
  filter_size: 3
  filter_count: 16
  activation: leaky_relu
- type: upscale
  method: nearest_neighbors
  shape: [28, 28]
- type: deconv
  filter_size: 9
  filter_count: 1
# flatten so it works like before. later i can change it so the loss accepts squares!
- type: reshape
  shape: [-1, 784]
  id: decoder_output

loss:
  type: reconstruction
  decoder_output_id: decoder_output
  softmax_output_id: softmax
  reconstruction_weight: 0.01
prediction_id: prediction
