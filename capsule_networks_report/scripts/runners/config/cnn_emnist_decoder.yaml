data_provider_name: emnist
batch_size: 50
epochs: 100
early_stop_after_n_epochs: 10
optimizer_kwargs:
  learning_rate: 0.0001
layers:
- type: conv
  filter_count: 200
  kernel_length: 5
  strides: 1
  activation: leaky_relu
- type: batch_norm
- type: dropout
  rate: 0.5
- type: conv
  filter_count: 200
  kernel_length: 3
  strides: 1
  activation: leaky_relu
- type: batch_norm
- type: dropout
  rate: 0.5
- type: max_pool
  pool_size: [2, 2]
  strides: 2
- type: conv
  filter_count: 400
  kernel_length: 3
  strides: 1
  activation: leaky_relu
- type: batch_norm
- type: conv
  filter_count: 400
  kernel_length: 5
  strides: 1
  activation: leaky_relu
- type: batch_norm
- type: max_pool
  pool_size: [2, 2]
  strides: 2
- type: reshape
  shape: [50, 1600]
- type: dense
  units: 450   # 700
  activation: leaky_relu
- type: dense
  units: 900   # 1400
  activation: leaky_relu
- type: dense
  units: 16
  activation: sigmoid
  id: mask
- type: softmax_pred
  num_classes: 10
  id: softmax
- type: argmax
  id: prediction

# do a dense layer so the mask can distribute
- input_id: mask
  type: dense
  units: 256
  activation: leaky_relu
- type: dense
  units: 784
  activation: leaky_relu
# flatten so it works like before. later i can change it so the loss accepts squares!
- type: reshape
  shape: [50, 784]
  id: flattened_decoder_output

loss:
  type: reconstruction
  decoder_output_id: flattened_decoder_output
  softmax_output_id: softmax
  reconstruction_weight: 0.0005
prediction_id: prediction
